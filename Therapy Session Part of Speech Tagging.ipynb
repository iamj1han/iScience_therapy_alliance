{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#             Notebook #2\n",
    "#                        Generate Part of Speech tags ad transition matrix \n",
    "#\n",
    "#\n",
    "#  Read (or reconstruct) the POS file containing all the POS for each sentence\n",
    "#  Compute word frequencies\n",
    "#  Construct the transition matrix and the histograms\n",
    "#  Generate Honore and Brunete stats for:\n",
    "#         Everything\n",
    "#         Pronouns \n",
    "#\n",
    "#  Standardized terms:\n",
    "#       N: Total numbers of words emitted\n",
    "#       V: Number of unique words (vocabulary)\n",
    "#       V1: Number of words used once\n",
    "#       P: Number of pronouns emitted\n",
    "#       PV: Number of unique pronouns (pronoun vocabulary)\n",
    "#       PV1: Number of pronouns ised once\n",
    "#\n",
    "#  Caveat Lector: \n",
    "#   This code runs on a specific directory structure. Yours will be different and so will require some\n",
    "#   refactoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#        Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import stanza \n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from dateutil.parser import parse\n",
    "import scipy as sp\n",
    "from sklearn.preprocessing import scale\n",
    "import numpy as np\n",
    "import math\n",
    "import sys,os\n",
    "import gc\n",
    "import phenograph \n",
    "#import networkx as nx\n",
    "import igraph as ig\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import scipy.stats as stats\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns; sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from matplotlib.ticker import *\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import MO, TU, WE, TH, FR, SA, SU\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "from sklearn import feature_selection\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import FastICA, PCA\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn import decomposition\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from docx.api import Document\n",
    "import re\n",
    "import string\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "#      Options\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "pkg_resources.get_distribution(\"stanza\").version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions and Classes\n",
    "\n",
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass\n",
    "\n",
    "from notebook.services.config import ConfigManager\n",
    "c = ConfigManager()\n",
    "c.update('notebook', {\"CodeCell\": {\"cm_config\": {\"autoCloseBrackets\": False}}})\n",
    "c.update('notebook', {\"CodeCell\": {\"cm_config\": {\"autoCloseQuotes\": False}}})\n",
    "\n",
    "#Figure out what variables are taking up memory\n",
    "def varSizes():\n",
    "    local_vars = list(locals().items())\n",
    "    print(local_vars)\n",
    "    varDF = pd.DataFrame(columns=['var','size'])\n",
    "    for var, obj in local_vars:\n",
    "        varDF.loc[len(varDF)]=[var, sys.getsizeof(obj)]\n",
    "    varDF.sort_values(by=['size'],inplace=True,ascending=False)\n",
    "    varDF.reset_index(inplace=True,drop=True)\n",
    "    return varDF\n",
    "    \n",
    "\n",
    "def docToDF(doc):\n",
    "    # Input: A Stanza doc object\n",
    "    # Output: DataFrame object with one row for each token in\n",
    "\n",
    "    #List of row data dictionaries\n",
    "    rows = []\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            row = {\n",
    "                \"text\": word.text,\n",
    "                \"lemma\": word.lemma,\n",
    "                \"upos\": word.upos,\n",
    "                \"xpos\": word.xpos,\n",
    "                \"deprel\": word.deprel,\n",
    "            }\n",
    "            rows.append(row)\n",
    "    return pd.DataFrame(rows)\n",
    "    \n",
    "def parse_SessionSpeaker(transcriptFileName):\n",
    "        #Full Transcript pathname version\n",
    "        if transcriptFileName[0]=='/':\n",
    "            path_file = os.path.split(transcriptFileName) \n",
    "            fileName = path_file[1]\n",
    "        #LIWC filename version\n",
    "        else:\n",
    "            fileName = transcriptFileName\n",
    "        \n",
    "        fileName = fileName.split('.')[0]\n",
    "        print('fileName: ', fileName)\n",
    "        fileParts = fileName.split('_')\n",
    "\n",
    "        Session = fileParts[0]\n",
    "        print('Session : ', Session)\n",
    "        \n",
    "        Speaker = fileParts[1]\n",
    "        print('Speaker : ', Speaker)\n",
    "        \n",
    "        return(Session,Speaker)\n",
    "\n",
    "def ngramStats(allTextList,n):\n",
    "    #Compute word frequencies\n",
    "    word_fdist = nltk.FreqDist(allTextList)    \n",
    "    wordDF = pd.DataFrame.from_dict(word_fdist, orient='index')\n",
    "    wordDF['word'] = wordDF.index\n",
    "    wordDF.columns = ['count','word']  \n",
    "    wordDF = wordDF[['word','count']] \n",
    "    wordDF.sort_values(by=['count'], ascending=False, inplace=True)\n",
    "    wordDF.reset_index(inplace=True,drop=True)\n",
    "    wordDF.head()\n",
    "\n",
    "    #Compute n gram frequencies\n",
    "    ngram_fd = nltk.FreqDist(ngrams(allTextList,n))\n",
    "    ngramDF = pd.DataFrame(ngram_fd.most_common(), columns =['w1-wn', 'Freq']) \n",
    "    return(wordDF,ngramDF)\n",
    " \n",
    "def removeNonTerminatingPunctuation(allPOSDF):\n",
    "    # Remove all non-terminating punctuation\n",
    "    allPOSDF = allPOSDF.loc[allPOSDF['text']!=',',:]\n",
    "    allPOSDF = allPOSDF.loc[allPOSDF['text']!=':',:]\n",
    "    allPOSDF = allPOSDF.loc[allPOSDF['text']!=';',:]\n",
    "    allPOSDF = allPOSDF.loc[allPOSDF['text']!='--',:]\n",
    "    allPOSDF = allPOSDF.loc[allPOSDF['text']!='/',:]\n",
    "    allPOSDF = allPOSDF.loc[allPOSDF['text']!='\"',:]\n",
    "    allPOSDF = allPOSDF.loc[allPOSDF['text']!='[',:]\n",
    "    allPOSDF = allPOSDF.loc[allPOSDF['text']!=']',:] \n",
    "    allPOSDF = allPOSDF.loc[allPOSDF['text']!='-',:]\n",
    "    allPOSDF = allPOSDF.loc[allPOSDF['text']!='(',:]\n",
    "    allPOSDF = allPOSDF.loc[allPOSDF['text']!=')',:]\n",
    "    allPOSDF = allPOSDF.loc[allPOSDF['text']!='“',:]\n",
    "    allPOSDF = allPOSDF.loc[allPOSDF['text']!=\"'\",:]\n",
    "    allPOSDF = allPOSDF.loc[allPOSDF['text']!='”',:]\n",
    "    return(allPOSDF)\n",
    "\n",
    "#Populate the turnDF and posDF\n",
    "def create_turnDF(all_text):\n",
    "    turnDF = pd.DataFrame(columns=['turn','sentence','text'])\n",
    "    turn = 0\n",
    "    posDF = pd.DataFrame(columns=['sentence','text','lemma','upos','xpos','deprel'])\n",
    "    sentenceTotalNum = 0\n",
    "    all_text = '.'+all_text\n",
    "    all_text = all_text.replace('\\n','.\\n')\n",
    "    currentParts = all_text.split(\"\\n\")\n",
    "    for i in range(len(currentParts)):\n",
    "            text = currentParts[i]\n",
    "            #                         Sentencize and Parse with stanza\n",
    "            #Note once you get the text into a Stanza doc you can do all sorts of interesting things\n",
    "            doc = nlp(text)\n",
    "\n",
    "            sentenceNum = 0\n",
    "            \n",
    "            for sentence in doc.sentences:\n",
    "               turnRow = [turn,sentenceNum,sentence.text]\n",
    "               turnDF.loc[len(turnDF)+1] = turnRow\n",
    "               \n",
    "               #Populate the POS \n",
    "               for word in sentence.words:\n",
    "                   posDF.loc[len(posDF)+1] = [sentenceTotalNum, word.text, word.lemma, word.upos, word.xpos, word.deprel]\n",
    "               sentenceNum = sentenceNum + 1\n",
    "               sentenceTotalNum = sentenceTotalNum + 1\n",
    "               \n",
    "            turn = turn + 1\n",
    "    turnDF.reset_index(inplace=True,drop=True)     \n",
    "    return [turnDF, posDF]\n",
    "\n",
    "\n",
    "#           Calculate Honore's statistic\n",
    "#Honore’s statistic [21] is based on the notion that the\n",
    "#larger the number of words used by a speaker that occur\n",
    "#only once, the richer his overall lexicon is. Words spoken\n",
    "#only once (V1) and the total vocabulary used (V) have\n",
    "#been shown to be linearly associated. Honore’s statistic\n",
    "#generates a lexical richness measure according to R =\n",
    "#100×log(N/(1−V1/V)), where N is the total text length.\n",
    "#Higher values correspond to a richer vocabulary. As with\n",
    "#standardized word entropy, stemming is done on words\n",
    "#and only the stems are considered.\n",
    "def honore(speakerDF):\n",
    "    #Find words (drop punctuation)\n",
    "    wordDF = speakerDF.loc[(speakerDF['upos']!='PUNCT'),:]\n",
    "    wordList = list(wordDF['lemma'].dropna())\n",
    "    numWords = len(wordList)\n",
    "    wordCounter = Counter(wordList)\n",
    "    wordFreq = wordCounter.most_common()\n",
    "    #Display the word frequency\n",
    "    oneTimeWords = []\n",
    "    for word, count in wordCounter.most_common():\n",
    "        #print(word,'\\t', count*1.0/numWords*1.0)\n",
    "        #print(word,'\\t', count)\n",
    "        if count==1:\n",
    "            oneTimeWords.append(word)\n",
    "    uniqueWordList = list(set(wordList))\n",
    "    numUniqueWords = len(uniqueWordList)\n",
    "    print('numUniqueWords: ', numUniqueWords)\n",
    "    numOneTimeWords = len(oneTimeWords)\n",
    "    print('numOneTimeWords: ',numOneTimeWords)\n",
    "    D = 1.0-numOneTimeWords/numUniqueWords\n",
    "    R = 100.0*(math.log(numWords/D,100))\n",
    "    return(R)\n",
    "\n",
    "#           Calculate Brunet’s index\n",
    "#Brunet’s index (W) quantifies lexical richness [20]. It is\n",
    "#calculated as W = NV**−0.165 , where N is the total text\n",
    "#length and V is the total vocabulary. Lower values of W\n",
    "#correspond to richer texts. As with standardized word\n",
    "#entropy, stemming is done on words and only the stems\n",
    "#are considered.\n",
    "def brunet(speakerDF):\n",
    "    #Find words (drop punctuation)\n",
    "    wordDF = speakerDF.loc[(speakerDF['upos']!='PUNCT'),:]\n",
    "    wordList = list(wordDF['lemma'].dropna())\n",
    "    numWords = len(wordList)\n",
    "    wordCounter = Counter(wordList)\n",
    "    wordFreq = wordCounter.most_common()\n",
    "    #Display the word frequency\n",
    "    oneTimeWords = []\n",
    "    for word, count in wordCounter.most_common():\n",
    "        #print(word,'\\t', count*1.0/numWords*1.0)\n",
    "        #print(word,'\\t', count)\n",
    "        if count==1:\n",
    "            oneTimeWords.append(word)\n",
    "    uniqueWordList = list(set(wordList))\n",
    "    numUniqueWords = len(uniqueWordList)\n",
    "    W = (numWords*numUniqueWords)**-0.165\n",
    "    return(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                                          Pull in models\n",
    "\n",
    "stanza.download('en') # download English model\n",
    "#Initialize a Stanza Pipeline\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,lemma,pos,depparse,ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"                                    Penn Treebank Project POS Tags\n",
    "Number\n",
    "Tag\n",
    "Description\n",
    "1.\tCC\tCoordinating conjunction\n",
    "2.\tCD\tCardinal number\n",
    "3.\tDT\tDeterminer\n",
    "4.\tEX\tExistential there\n",
    "5.\tFW\tForeign word\n",
    "6.\tIN\tPreposition or subordinating conjunction\n",
    "7.\tJJ\tAdjective\n",
    "8.\tJJR\tAdjective, comparative\n",
    "9.\tJJS\tAdjective, superlative\n",
    "10.\tLS\tList item marker\n",
    "11.\tMD\tModal\n",
    "12.\tNN\tNoun, singular or mass\n",
    "13.\tNNS\tNoun, plural\n",
    "14.\tNNP\tProper noun, singular\n",
    "15.\tNNPS\tProper noun, plural\n",
    "16.\tPDT\tPredeterminer\n",
    "17.\tPOS\tPossessive ending\n",
    "18.\tPRP\tPersonal pronoun\n",
    "19.\tPRP$\tPossessive pronoun\n",
    "20.\tRB\tAdverb\n",
    "21.\tRBR\tAdverb, comparative\n",
    "22.\tRBS\tAdverb, superlative\n",
    "23.\tRP\tParticle\n",
    "24.\tSYM\tSymbol\n",
    "25.\tTO\tto\n",
    "26.\tUH\tInterjection\n",
    "27.\tVB\tVerb, base form\n",
    "28.\tVBD\tVerb, past tense\n",
    "29.\tVBG\tVerb, gerund or present participle\n",
    "30.\tVBN\tVerb, past participle\n",
    "31.\tVBP\tVerb, non-3rd person singular present\n",
    "32.\tVBZ\tVerb, 3rd person singular present\n",
    "33.\tWDT\tWh-determiner\n",
    "34.\tWP\tWh-pronoun\n",
    "35.\tWP$\tPossessive wh-pronoun\n",
    "36.\tWRB\tWh-adverb\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#            Read transcripts for each video diary session and create a Pos file (if it doesn't already exist)\n",
    "#\n",
    "# Only need to do this once, unless you are changing something...\n",
    "#\n",
    "transcriptDir = '/Users/Heisig/Jihan/Transcripts/SplitTranscripts/'\n",
    "posDir = '/Users/Heisig/Jihan/POS/'\n",
    "resultsDir = '/Users/Heisig/Jihan/Results/'\n",
    "liwcDyadFileName = '/Users/Heisig/Jihan/Results/liwcDyadData.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                     allEmbeddingsDF processing\n",
    "embDir = '/Users/Heisig/Jihan/Embeddings/'\n",
    "#Read the embedding file...\n",
    "allEmbeddingFileName = resultsDir+'AllEmbeddings.csv'\n",
    "\n",
    "if os.path.isfile(allEmbeddingFileName):\n",
    "   allEmbeddingsDF = pd.read_csv(allEmbeddingFileName)\n",
    "else:\n",
    "    #Build a list of all the embed files \n",
    "    emb_file_paths = []  # List which will store all of the full filepaths.\n",
    "    # Walk the directory tree.\n",
    "    for root, directories, files in os.walk(embDir):\n",
    "        for filename in files:\n",
    "            # Join the two strings in order to form the full filepath.\n",
    "            filepath = os.path.join(root, filename)\n",
    "            if 'Embedding.csv' in filepath:\n",
    "                emb_file_paths.append(filepath)  # Add it to the list.\n",
    "    emb_file_paths = sorted(emb_file_paths)\n",
    "    print('emb_file_paths: \\n',*list(emb_file_paths),sep='\\n')\n",
    "\n",
    "    allEmbeddingsDF = pd.DataFrame()\n",
    "\n",
    "    for embFileName in emb_file_paths:\n",
    "        #Read the Embedding file\n",
    "        print('Reading: :',embFileName)\n",
    "        embDF = pd.read_csv(embFileName) \n",
    "        embDF['sentence'] = embDF['sentence'] + sentenceCursor\n",
    "        sentenceCursor = np.max(embDF['sentence'])+1\n",
    "        allEmbeddingsDF = allEmbeddingsDF.append(embDF, ignore_index=True)\n",
    "\n",
    "    allEmbeddingsDF.reset_index(inplace=True,drop=True)\n",
    "    allEmbeddingsDF.to_csv(allEmbeddingFileName, index=False, header=True, sep=',')\n",
    "    print('Saved: ',allEmbeddingFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allEmbeddingsDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(set(allEmbeddingsDF.subjectsession)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#             Read all the Parts of Speech files into a single data frame and save to a file (if it doesn't already exist...)\n",
    "\n",
    "# Only need to do this once, unless you are changing something...\n",
    "allPOSFileName = resultsDir+'AllPOS.csv'\n",
    "sentenceCursor = 0\n",
    "\n",
    "if os.path.isfile(allPOSFileName):\n",
    "   allPOSDF = pd.read_csv(allPOSFileName)\n",
    "   print('Reading: ',allPOSFileName)\n",
    "else:\n",
    "    #Build a list of all the pos files \n",
    "    pos_file_paths = []  # List which will store all of the full filepaths.\n",
    "    # Walk the directory tree.\n",
    "    for root, directories, files in os.walk(posDir):\n",
    "        for filename in files:\n",
    "            # Join the two strings in order to form the full filepath.\n",
    "            filepath = os.path.join(root, filename)\n",
    "            if '_POS' in filepath:\n",
    "                pos_file_paths.append(filepath)  # Add it to the list.\n",
    "    pos_file_paths = sorted(pos_file_paths)\n",
    "    print('pos_file_paths: \\n',*list(pos_file_paths),sep='\\n')\n",
    "\n",
    "    allPOSDF = pd.DataFrame()\n",
    "\n",
    "    for posFileName in pos_file_paths:\n",
    "        #Read the POS file\n",
    "        print('Reading: :',posFileName)\n",
    "        posDF = pd.read_csv(posFileName) \n",
    "        posDF['sentence'] = posDF['sentence'] + sentenceCursor\n",
    "        sentenceCursor = np.max(posDF['sentence'])+1\n",
    "        allPOSDF = allPOSDF.append(posDF, ignore_index=True)\n",
    "        \n",
    "    allPOSDF.reset_index(inplace=True,drop=True)\n",
    "    allPOSDF.to_csv(allPOSFileName, index=False, header=True, sep=',')\n",
    "    print('Saved: ',allPOSFileName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(set(allPOSDF.subjectsession)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                                    Build some indices\n",
    "\n",
    "#Find the sentence numbers of all questions\n",
    "punctAllDF = allPOSDF.groupby('sentence').last()\n",
    "punctAllDF.reset_index(inplace=True)\n",
    "questionsIdx = punctAllDF.loc[punctAllDF.text=='?','sentence']\n",
    "#Find the sentence number of all NOT questions\n",
    "notQuestionsIdx = punctAllDF.loc[punctAllDF.text!='?','sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the set of ALL pronouns xpos version\n",
    "xposPronDF = allPOSDF.loc[allPOSDF['xpos']=='PRP',:]\n",
    "xposSet = list(set( xposPronDF['lemma'].str.lower()))\n",
    "xposList = list(xposPronDF['lemma'].str.lower())\n",
    "\n",
    "#Find the set of ALL pronouns upos version\n",
    "uposAllPronDF = allPOSDF.loc[allPOSDF['upos']=='PRON',:]\n",
    "uposAllPronounList = list(uposAllPronDF['lemma'].str.lower())\n",
    "uposAllPronounList = [item for item in uposAllPronounList if item!='covid']\n",
    "uposAllPronounSet = list(set(uposAllPronounList))\n",
    "#Session Speaker\n",
    "sortedPronounList = [item for items, c in Counter(uposAllPronounList).most_common() for item in [items] * c]\n",
    "pronounFrequencyDict = Counter(sortedPronounList)\n",
    "print(pronounFrequencyDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commonPronounList = ['i', 'you', 'it', 'that', 'she', 'he', 'my', 'they', 'what', 'we', 'this', 'something', 'there', 'which', 'myself', 'who', 'anything', 'everything', 'yourself', 'whatever', 'somebody', 'us', 'someone', 'nothing', 'everybody', 'nobody', 'anybody', 'anyone', 'everyone', 'itself', 'mine', 'its', 'herself', 'himself', 'whose', 'themselves', 'yours', 'one', 'whoever', 'ourselves', 'her',\n",
    "'his']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute ALL word frequency\n",
    "wordDF = allPOSDF.loc[allPOSDF['upos']!='PUNCT',:]\n",
    "words = list(wordDF['lemma'].dropna())\n",
    "totalWords = len(words)\n",
    "wordCounter = Counter(words)\n",
    "wordFreq = wordCounter.most_common()\n",
    "#Display the word frequency\n",
    "for word, count in wordCounter.most_common(100):\n",
    "    print(word,'\\t', count*1.0/totalWords*1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                                             Subset allPOSDF to a single session/speaker\n",
    "\n",
    "#      Pick a session and speaker\n",
    "#session = 31320  #Lowest Patient Alliance   \n",
    "session = 101520  #Highest Patient Alliance \n",
    "\n",
    "speaker = 'therapist'\n",
    "speakerDF = allPOSDF.loc[(allPOSDF['subjectsession']==session) & (allPOSDF['speaker']==speaker),:].copy()\n",
    "speakerDF.reset_index(inplace=True,drop=True)\n",
    "print(speakerDF.head())\n",
    "speakerDF['tokenNum'] = speakerDF.index\n",
    "#Realign the sentence number\n",
    "speakerDF['speakerSentenceNum'] = speakerDF['sentence']\n",
    "speakerDF['speakerSentenceNum'] = speakerDF['speakerSentenceNum'] - speakerDF.loc[0,'speakerSentenceNum']\n",
    "print(speakerDF.shape)\n",
    "\n",
    "uposPronDF = speakerDF.loc[(speakerDF['upos']=='PRON'),:]\n",
    "uposPronounList = list(uposPronDF['lemma'].str.lower())\n",
    "uposPronounList = [item for item in uposPronounList if item!='covid']\n",
    "uposPronounSet = list(set(uposPronounList))\n",
    "sortedPronounList = [item for items, c in Counter(uposPronounList).most_common() for item in [items] * c]\n",
    "\n",
    "\n",
    "uposIntjDF = speakerDF.loc[(speakerDF['upos']=='INTJ'),:]\n",
    "uposIntjList = list(uposIntjDF['lemma'].str.lower())\n",
    "uposIntjList = [item for item in uposIntjList if item!='covid']\n",
    "uposIntjSet = list(set(uposIntjList))\n",
    "sortedIntjList = [item for items, c in Counter(uposIntjList).most_common() for item in [items] * c]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "allPOSDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                                                   Plot Pronouns\n",
    "sns.set_style('darkgrid')\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(16,16)\n",
    "\n",
    "#Plot Histogram\n",
    "sns.histplot(sortedPronounList, bins=len(sortedPronounList), label='Pronouns ' , color='limegreen')\n",
    "\n",
    "#Title and Axis labels  \n",
    "ax.set_title('  Distribution of Pronouns '+str(session)+' '+speaker,fontsize='xx-large')    \n",
    "\n",
    "plt.legend(fontsize='x-large', title_fontsize='40')\n",
    "plt.setp(ax.get_xticklabels(), rotation=30, fontsize='medium');\n",
    "\n",
    "plt.xlabel('Pronoun',fontsize='x-large', weight='bold')\n",
    "plt.ylabel('Observations',fontsize='x-large', weight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Session Speaker\n",
    "pronounFrequencyDict = Counter(sortedPronounList)\n",
    "print(pronounFrequencyDict)\n",
    "\n",
    "intjFrequencyDict = Counter(sortedIntjList)\n",
    "print(intjFrequencyDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the session speaker word frequency\n",
    "wordDF = speakerDF.loc[speakerDF['upos']!='PUNCT',:]\n",
    "words = list(wordDF['lemma'].dropna())\n",
    "totalWords = len(words)\n",
    "print(totalWords,' totalwords')\n",
    "wordCounter = Counter(words)\n",
    "wordFreq = wordCounter.most_common()\n",
    "#Display the word frequency\n",
    "for word, count in wordCounter.most_common(100):\n",
    "    print(word,'\\t', count*1.0/totalWords*1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count/totalWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the upos token frequency\n",
    "tokens = list(speakerDF['upos'].dropna())\n",
    "tokenCounter = Counter(tokens)\n",
    "tokenFreq = tokenCounter.most_common()\n",
    "#Display the word frequency\n",
    "for token, count in tokenCounter.most_common(100):\n",
    "    print('%s %7d' % (token, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allPOSDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                                                   Plot UPOS tokens\n",
    "sns.set_style('darkgrid')\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(16,16)\n",
    "\n",
    "transitions = speakerDF['upos']\n",
    "tokenCounter = Counter(transitions)\n",
    "uposTokens = tokenCounter.most_common()\n",
    "\n",
    "uposTokens = [item for items, c in Counter(transitions).most_common() for item in [items] * c]\n",
    "\n",
    "#Plot Histogram\n",
    "sns.histplot(uposTokens, bins=17, label='UPOS ' , color='limegreen')\n",
    "\n",
    "#Title and Axis labels  \n",
    "ax.set_title('  Distribution of UPOS tokens '+str(session)+' '+speaker,fontsize='xx-large')    \n",
    "\n",
    "plt.legend(fontsize='x-large', title_fontsize='40')\n",
    "#tickPositions = list(range(10))\n",
    "#tickLabels = list(range(10))\n",
    "#plt.xticks(tickPositions,tickLabels,fontsize=35)\n",
    "plt.setp(ax.get_xticklabels(), rotation=30, fontsize='medium');\n",
    "\n",
    "plt.xlabel('UPOS',fontsize='x-large', weight='bold')\n",
    "plt.ylabel('Observations',fontsize='x-large', weight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the xpos token frequency\n",
    "tokens = list(speakerDF['xpos'].dropna())\n",
    "xposList = list(set(tokens))\n",
    "tokenCounter = Counter(tokens)\n",
    "tokenFreq = tokenCounter.most_common()\n",
    "#Display the word frequency\n",
    "for token, count in tokenCounter.most_common(100):\n",
    "    print('%s %7d' % (token, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                                                   Plot XPOS tokens\n",
    "sns.set_style('darkgrid')\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(16,16)\n",
    "\n",
    "transitions = speakerDF['xpos']\n",
    "tokenCounter = Counter(transitions)\n",
    "uposTokens = tokenCounter.most_common()\n",
    "\n",
    "uposTokens = [item for items, c in Counter(transitions).most_common() for item in [items] * c]\n",
    "\n",
    "#Plot Histogram\n",
    "sns.histplot(uposTokens, bins=len(xposList), label='XPOS ' , color='limegreen')\n",
    "\n",
    "#Title and Axis labels  \n",
    "ax.set_title('  Distribution of XPOS tokens '+str(session)+' '+speaker,fontsize='xx-large')    \n",
    "\n",
    "plt.legend(fontsize='x-large', title_fontsize='40')\n",
    "#tickPositions = list(range(10))\n",
    "#tickLabels = list(range(10))\n",
    "#plt.xticks(tickPositions,tickLabels,fontsize=35)\n",
    "plt.setp(ax.get_xticklabels(), rotation=30, fontsize='medium');\n",
    "\n",
    "plt.xlabel('XPOS',fontsize='x-large', weight='bold')\n",
    "plt.ylabel('Observations',fontsize='x-large', weight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         Compute and Plot the UPOS Transition matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The set of upos tokens (states)\n",
    "uposList = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
    "uposNum = len(uposList)\n",
    "len(uposList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                        Function to calculate the transition matrix and transition vector for all speakerDFs\n",
    "\n",
    "def calcTranDF(speakerDF):\n",
    "    #Compute the transition matrix for the upos tokens\n",
    "    def calcRowProb(row):\n",
    "        s = sum(row)\n",
    "        if s > 0:\n",
    "            row[:] = [f/s for f in row]\n",
    "        return row\n",
    "\n",
    "    #The set of upos tokens (states)\n",
    "    uposList = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
    "    uposNum = len(uposList)\n",
    "    \n",
    "    #Define the transition matrix\n",
    "    transDF = pd.DataFrame(np.zeros((uposNum,uposNum)))\n",
    "\n",
    "    #Define the element to index dictionary\n",
    "    allUposDict = {k: v for v, k in enumerate(uposList)}\n",
    "    #print(allUposDict)\n",
    "\n",
    "    idDF = pd.DataFrame(np.zeros((uposNum,uposNum)))\n",
    "    \n",
    "    #Get rid of internal sentence punctuation\n",
    "    cleanPOSDF = removeNonTerminatingPunctuation(speakerDF)\n",
    "    transitionChars = cleanPOSDF['upos']\n",
    "\n",
    "    #Apply the dictionary to switch from upos tokens to indices\n",
    "    transitions = [allUposDict[k] for k in transitionChars]\n",
    "\n",
    "    #Count the transitions (this needs to be a list so we can look at it later, a zip obj gets 'consumed')\n",
    "    transitionTupleList = list(zip(transitions,transitions[1:]))\n",
    "    transitionCharTupleList = list(zip(transitionChars,transitionChars[1:]))\n",
    "    print('transitionCharTupleList: \\n',transitionCharTupleList)\n",
    "    for (i,j) in transitionTupleList:\n",
    "            transDF.iloc[i,j] += 1\n",
    "\n",
    "    #Convert the counts to probabilities:        \n",
    "    transDF.apply(calcRowProb, axis=1)\n",
    "\n",
    "    #Add column and row names\n",
    "    transDF.columns = uposList\n",
    "    transDF.index = uposList\n",
    "    \n",
    "    idDF = pd.DataFrame(columns=uposList,index=uposList)\n",
    "    for c in uposList:\n",
    "        for r in uposList:\n",
    "            idDF.at[r,c] = r+','+c\n",
    "\n",
    "    #Extract the transition vector from the triangular matrix (including the diagonal!!!)\n",
    "    rows,cols = transDF.shape\n",
    "    transNP = transDF.to_numpy()\n",
    "    transVector = transNP[np.triu_indices(rows, k = 0)]\n",
    "    transVector = pd.Series(transVector)\n",
    "    \n",
    "    #Figure out the names and label the transition vector\n",
    "    idDF.columns=range(uposNum)\n",
    "    idDF.index=range(uposNum)\n",
    "    indices = np.triu_indices(uposNum, k = 0)\n",
    "    tupleList = list(zip(indices[0],indices[1]))\n",
    "    vectorIdList = [] \n",
    "    for t in tupleList:\n",
    "        vectorIdList.append(idDF.at[t]) \n",
    "    transVector.index = vectorIdList\n",
    "    \n",
    "    return(transDF,transVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#                               Calculate and save the pronoun vectors for all the speaker/session pairs\n",
    "\n",
    "# Values are:  Frequency I/You Self/Other, frequency of each pronoun wrt all speech\n",
    "\n",
    "\n",
    "commonPronounList = ['i', 'you', 'it', 'that', 'she', 'he', 'my', 'they', 'what', 'we', 'this', 'something', 'there', 'which', 'myself', 'who', 'anything', 'everything', 'yourself', 'whatever', 'somebody', 'us', 'someone', 'nothing', 'everybody', 'nobody', 'anybody', 'anyone', 'everyone', 'itself', 'mine', 'its', 'herself', 'himself', 'whose', 'themselves', 'yours', 'one', 'whoever', 'ourselves', 'her',\n",
    "'his'] \n",
    "pronounsPrefixedList =  ['pronoun_' + pronoun for pronoun in commonPronounList]\n",
    "allSessions = list(set(allPOSDF['subjectsession']))\n",
    "speakers = ['therapist','patient']\n",
    "transDir = '/Users/Heisig/Jihan/POS_Transitions/'\n",
    "resultsDir = '/Users/Heisig/Jihan/Results/'\n",
    "vectorList = []\n",
    "sampleList = []\n",
    "\n",
    "columnList = ['Session', 'Speaker', 'pronoun_Honore', 'pronoun_Brunet', 'pronoun_IY', 'pronoun_SO', 'pronoun_TP', 'pronoun_WE']+commonPronounList\n",
    "finalColumnList = ['Session', 'Speaker', 'pronoun_Honore', 'pronoun_Brunet', 'pronoun_IY', 'pronoun_SO', 'pronoun_TP', 'pronoun_WE']+pronounsPrefixedList\n",
    "allPronounDF = pd.DataFrame(columns = columnList)\n",
    "\n",
    "for session in allSessions:\n",
    "    for speaker in speakers:\n",
    "        print(session,speaker)\n",
    "        sampleList.append(str(session)+'-'+speaker)\n",
    "        speakerDF = allPOSDF.loc[(allPOSDF['subjectsession']==session) & (allPOSDF['speaker']==speaker),:].copy()\n",
    "        speakerDF.reset_index(inplace=True,drop=True)\n",
    "        speakerDF['tokenNum'] = speakerDF.index\n",
    "        \n",
    "        \n",
    "        #Find the set of ALL pronouns upos version\n",
    "        uposAllPronDF = speakerDF.loc[speakerDF['upos']=='PRON',:]\n",
    "        uposAllPronounList = list(uposAllPronDF['lemma'].str.lower())\n",
    "        uposAllPronounList = [item for item in uposAllPronounList if item!='covid']\n",
    "        uposAllPronounSet = list(set(uposAllPronounList))\n",
    "        #Session Speaker\n",
    "        sortedPronounList = [item for items, c in Counter(uposAllPronounList).most_common() for item in [items] * c]\n",
    "        pronounFrequencyDict = Counter(sortedPronounList)\n",
    "        #print(pronounFrequencyDict)\n",
    "        \n",
    "        #Calculate Honore's statistic\n",
    "        R = honore(uposAllPronDF)\n",
    "        print(\"Honore's statistic: \",R)\n",
    "        \n",
    "        #Calculate Honore's statistic\n",
    "        W = brunet(uposAllPronDF)\n",
    "        print(\"Brunet's Index: \",W)\n",
    "        \n",
    "        #Compute pronoun frequencies\n",
    "        rows,cols = speakerDF.shape\n",
    "        pronounVec = [0]*len(commonPronounList)\n",
    "        idx = 0\n",
    "        for pronoun in commonPronounList:\n",
    "            if pronoun in pronounFrequencyDict:\n",
    "                count = pronounFrequencyDict[pronoun]/float(rows)\n",
    "                pronounVec[idx] = count\n",
    "            idx = idx + 1\n",
    "\n",
    "        #Create the rowDF\n",
    "        #Init I/You, Self vs Other and Third Party\n",
    "        IY = 0\n",
    "        SO = 0\n",
    "        TP = 0\n",
    "        WE = 0\n",
    "        pronounRow = [session, speaker, R, W, IY, SO, TP, WE]+pronounVec\n",
    "        pronounRowDF = pd.DataFrame([pronounRow],columns=columnList)\n",
    "        \n",
    "        #Compute I/You and Self vs Other\n",
    "        pronounRowDF['pronoun_IY'] =  pronounRowDF['i']/pronounRowDF['you']\n",
    "        pronounRowDF['pronoun_SO'] = (pronounRowDF['i']+pronounRowDF['my']+pronounRowDF['myself'])/(pronounRowDF['you']+pronounRowDF['yours']+pronounRowDF['yourself'])\n",
    "        pronounRowDF['pronoun_TP'] =  pronounRowDF['she']+pronounRowDF['he']+pronounRowDF['they']+pronounRowDF['somebody']+pronounRowDF['someone']+pronounRowDF['everybody']+pronounRowDF['nobody']+pronounRowDF['anybody']+pronounRowDF['anyone']+pronounRowDF['everyone']\n",
    "        pronounRowDF['pronoun_WE'] =  pronounRowDF['we']+pronounRowDF['us']+pronounRowDF['ourselves']\n",
    "        #Mash this person's pronouns into the common DF  \n",
    "        allPronounDF = allPronounDF.append(pronounRowDF, ignore_index=True)  \n",
    "\n",
    "allPronounDF.columns = finalColumnList\n",
    "allPronounFileName = resultsDir + 'allPronounDF.csv' \n",
    "allPronounDF.to_csv(allPronounFileName, index=False, header=True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allPronounDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#                   Calculate the transition matrices and vectors for all the speaker/session pairs\n",
    "\n",
    "allSessions = list(set(allPOSDF['subjectsession']))\n",
    "speakers = ['therapist','patient']\n",
    "transDir = '/Users/Heisig/Jihan/POS_Transitions/'\n",
    "resultsDir = '/Users/Heisig/Jihan/Results/'\n",
    "\n",
    "allTranDF = pd.DataFrame()\n",
    "\n",
    "for session in allSessions:\n",
    "    for speaker in speakers:\n",
    "        #print(session,speaker)\n",
    "        speakerDF = allPOSDF.loc[(allPOSDF['subjectsession']==session) & (allPOSDF['speaker']==speaker),:].copy()\n",
    "        speakerDF.reset_index(inplace=True,drop=True)\n",
    "        speakerDF['tokenNum'] = speakerDF.index\n",
    "        #Realign the sentence number\n",
    "        speakerDF['speakerSentenceNum'] = speakerDF['sentence']\n",
    "        speakerDF['speakerSentenceNum'] = speakerDF['speakerSentenceNum'] - speakerDF.loc[0,'speakerSentenceNum']\n",
    "        #print(speakerDF.shape)\n",
    "        [transDF,transVec] = calcTranDF(speakerDF)\n",
    "        #Save the individual session/speaker transition matrix\n",
    "        transFileName = transDir + str(session) + '_' + speaker + '_tranDF.csv' \n",
    "        transDF.to_csv(transFileName, index=False, header=True, sep=',')\n",
    "        \n",
    "        tranRow = [session, speaker]\n",
    "        tranRow.extend(transVec.tolist())\n",
    "        \n",
    "        columnList = ['Session', 'Speaker']\n",
    "        columnList.extend(transVec.index.tolist())\n",
    "        \n",
    "        tranRowDF = pd.DataFrame([tranRow],columns=columnList)\n",
    "        allTranDF = allTranDF.append(tranRowDF, ignore_index=True)\n",
    "\n",
    "allTranFileName = resultsDir + 'allPOStransDF.csv' \n",
    "allTranDF.to_csv(allTranFileName, index=False, header=True, sep=',')\n",
    "allTranDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#                   Calculate the Honore and Brunete for all words for each speaker/session pair\n",
    "\n",
    "allSessions = list(set(allPOSDF['subjectsession']))\n",
    "#speakers = ['therapist','patient']\n",
    "speakers = ['therapist','patient']\n",
    "resultsDir = '/Users/Heisig/Jihan/Results/'\n",
    "vectorList = []\n",
    "for session in allSessions:\n",
    "    for speaker in speakers:\n",
    "        #print(session,speaker)\n",
    "        speakerDF = allPOSDF.loc[(allPOSDF['subjectsession']==session) & (allPOSDF['speaker']==speaker),:].copy()\n",
    "        speakerDF.reset_index(inplace=True,drop=True)\n",
    "        speakerDF['tokenNum'] = speakerDF.index\n",
    "        #Realign the sentence number\n",
    "        speakerDF['speakerSentenceNum'] = speakerDF['sentence']\n",
    "        speakerDF['speakerSentenceNum'] = speakerDF['speakerSentenceNum'] - speakerDF.loc[0,'speakerSentenceNum']\n",
    "        #print(speakerDF.shape)\n",
    "        \n",
    "        #Calculate Honore's statistic\n",
    "        R = honore(speakerDF)\n",
    "        #print(\"Honore's statistic: \",R)\n",
    "        W = brunet(speakerDF)\n",
    "        #print(\"Brunet's Index: \",W)\n",
    "        vectorList.append([session,speaker,R,W])\n",
    "        \n",
    "hbDF = pd.DataFrame(vectorList)\n",
    "hbDF.columns=['Session','Speaker','Honore','Brunet']\n",
    "\n",
    "hbFileName = resultsDir + 'honoreBrunetDF.csv' \n",
    "hbDF.to_csv(hbFileName, index=False, header=True, sep=',')\n",
    "hbDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Frequency of transitions\n",
    "transitionCharTupleList[0:25]\n",
    "#Compute the upos token frequency\n",
    "tokens = transitionCharTupleList\n",
    "tokenCounter = Counter(tokens)\n",
    "tokenFreq = tokenCounter.most_common()\n",
    "#Display the word frequency\n",
    "for token, count in tokenCounter.most_common(100):\n",
    "    print('%s %7d' % (token, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitionTupleList[0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the transition matrix\n",
    "plt.figure(figsize=(22,22))\n",
    "sns.heatmap(transDF, annot=False, cmap='coolwarm', xticklabels=uposList, yticklabels=uposList)\n",
    "plt.tick_params(axis='both', which='major', labelsize=15, labelbottom = False, bottom=False, top = False, labeltop=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows,cols = transDF.shape\n",
    "print(transDF.shape)\n",
    "print('total elements: ',rows*cols)\n",
    "transNP = transDF.to_numpy()\n",
    "transVector = transNP[np.triu_indices(rows, k = 1)]\n",
    "print('transVector length: ',len(transVector))\n",
    "print(transVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                      Evaluate a sentence using the transition matrix\n",
    "#  First: Sentencize and Parse with stanza\n",
    "\n",
    "rows = []\n",
    "\n",
    "#SVO example\n",
    "example_text = \".Let's stick to the plan.\"\n",
    "\n",
    "doc = nlp(example_text)\n",
    "\n",
    "sentenceTotalNum = 0\n",
    "for sentence in doc.sentences:\n",
    "   for word in sentence.words:\n",
    "            row = {\n",
    "                \"sentence\": sentenceTotalNum,\n",
    "                \"text\": word.text,\n",
    "                \"lemma\": word.lemma,\n",
    "                \"upos\": word.upos,\n",
    "                \"xpos\": word.xpos,\n",
    "                \"deprel\": word.deprel,\n",
    "            }\n",
    "            rows.append(row)\n",
    "   sentenceTotalNum = sentenceTotalNum + 1     \n",
    "\n",
    "sentenceDF = pd.DataFrame(rows) \n",
    "sentenceDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                                       Evaluate the sentence with the transition matrix\n",
    "# Basically run the upos sequence\n",
    "sentenceTokens = list(sentenceDF['upos'])\n",
    "print('sentenceText: \\n',sentenceDF['text'])\n",
    "print('sentenceTokens: ',sentenceTokens)\n",
    "sentenceSequenceTupleList = list(zip(sentenceTokens,sentenceTokens[1:]))\n",
    "#print('sentenceSequenceTupleList: ',sentenceSequenceTupleList)\n",
    "\n",
    "# Compute the perplexity for the test sentence given the 805 transition matrix\n",
    "# token transition probablity\n",
    "ttp = 1.0\n",
    "N = len(sentenceSequenceTupleList)\n",
    "print('N: ',N)\n",
    "for (i,j) in sentenceSequenceTupleList:\n",
    "        print(transDF[i][j])\n",
    "        ttp = ttp*transDF[i][j]\n",
    "print('total transition probability: ',ttp)\n",
    "\n",
    "#Normalize for sentence length\n",
    "ppx = (1.0/ttp)**(1.0/N)\n",
    "print('perplexity: ',ppx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Find sentences with a specific words\n",
    "targetLemma = 'I'\n",
    "#Find sentence numbers containing the target word\n",
    "targetDF = speakerDF.loc[(speakerDF['lemma']==targetLemma),:]\n",
    "targetDF.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Find sentences with various combinations of words and POS\n",
    "#Find sentence numbers containing the first target word\n",
    "speakerDF = allPOSDF.loc[allPOSDF['speaker']=='Interviewer',:].copy()\n",
    "targetLemma = 'I'\n",
    "targetDF = speakerDF.loc[(speakerDF['lemma']==targetLemma)&(speakerDF['deprel']=='nsubj'),:]\n",
    "targetSentenceNumberList = sorted(list(set(targetDF['sentence'])))\n",
    "#Show the list of sentences containing the target word in its specified role\n",
    "targetSentencesDF = speakerDF.loc[speakerDF['sentence'].isin(targetSentenceNumberList),:] \n",
    "print(targetSentencesDF)\n",
    "\n",
    "#Find sentence numbers containing the second target word\n",
    "targetLemma = 'you'\n",
    "targetDF = targetSentencesDF.loc[(targetSentencesDF['lemma']==targetLemma)&(targetSentencesDF['deprel']=='obj'),:]\n",
    "targetSentenceNumberList = sorted(list(set(targetDF['sentence'])))\n",
    "#Show the list of sentences containing the target word in its specified role\n",
    "targetSentencesDF = speakerDF.loc[speakerDF['sentence'].isin(targetSentenceNumberList),:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "targetDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakerDF = allPOSDF.loc[allPOSDF['speaker']=='Interviewer',:].copy()\n",
    "targetLemma = 'so'\n",
    "targetDF = speakerDF.loc[(speakerDF['lemma']==targetLemma),:]\n",
    "targetSentenceNumberList = sorted(list(set(targetDF['sentence'])))\n",
    "#Show the list of sentences containing the target word in its specified role\n",
    "targetSentencesDF = speakerDF.loc[speakerDF['sentence'].isin(targetSentenceNumberList),:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#targetSentencesDF\n",
    "#speakerDF[0:100]\n",
    "targetSentencesDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the plot\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(24, 8)\n",
    "\n",
    "ax = sns.scatterplot(data=targetDF,\n",
    "                     x='tokenNum',\n",
    "                     y='speakerSentenceNum',\n",
    "                     palette=\"deep\",\n",
    "                     hue='deprel',\n",
    "                     s=50,\n",
    "            # linewidth in a scatterplot removes the white circle\n",
    "                     linewidth=0,\n",
    "                     alpha=1.0)\n",
    "\n",
    "plt.xlabel('token position',fontsize='large', weight='bold')\n",
    "plt.ylabel('sentenceNum',fontsize='large', weight='bold')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recover all the sentence strings containing the target word\n",
    "targetSentenceList = []\n",
    "for s in targetSentenceNumberList:\n",
    "    sentenceDF = targetSentencesDF.loc[targetSentencesDF['sentence']==s]\n",
    "    sentence = ' '.join(sentenceDF['text'])\n",
    "    targetSentenceList.append(sentence)\n",
    "targetSentenceList    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull out instances of a POS transition\n",
    "#pos1 = 'AUX'\n",
    "#pos2 = 'INTJ'\n",
    "\n",
    "#pos1 = 'CCONJ'\n",
    "#pos2 = 'X'\n",
    "\n",
    "#pos1 = 'PRON'\n",
    "#pos2 = 'PRON'\n",
    "\n",
    "#pos1 = 'ADP'\n",
    "#pos2 = 'INTJ'\n",
    "\n",
    "\n",
    "pos1 = 'SCONJ'\n",
    "pos2 = 'PRON'\n",
    "\n",
    "posTranDF = pd.DataFrame(columns=['session','speaker','sentenceNum','POS1','POS2','sentenceText']) \n",
    "pos1DF = allPOSDF.loc[allPOSDF['upos']==pos1,:].copy()\n",
    "for idx in pos1DF.index:\n",
    "    if allPOSDF.loc[idx+1,'upos']==pos2:\n",
    "        sentenceNum = pos1DF.loc[idx,'sentence']\n",
    "        sentenceDF = allEmbeddingsDF.loc[allEmbeddingsDF['sentence']==sentenceNum,'text']\n",
    "        sentenceDF.reset_index(inplace=True,drop=True)\n",
    "        sentenceText = sentenceDF[0]\n",
    "        newRow = [pos1DF.loc[idx,'subjectsession'],pos1DF.loc[idx,'speaker'],sentenceNum,pos1DF.loc[idx,'text'],allPOSDF.loc[idx+1,'text'],sentenceText]\n",
    "        print('New row: ',newRow)\n",
    "        posTranDF.loc[len(posTranDF)] = newRow\n",
    "#targetDF = speakerDF.loc[(speakerDF['lemma']==targetLemma),:]\n",
    "#targetSentenceNumberList = sorted(list(set(targetDF['sentence'])))\n",
    "#Show the list of sentences containing the target word in its specified role\n",
    "#targetSentencesDF = speakerDF.loc[speakerDF['sentence'].isin(targetSentenceNumberList),:] \n",
    "posTranFileName = resultsDir + pos1+'_'+pos2+'_POS_transDF.csv' \n",
    "posTranDF.to_csv(posTranFileName, index=False, header=True, sep=',')\n",
    "posTranDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in posTranDF.iterrows():\n",
    "    sentence = row['sentence']\n",
    "    sentenceDF = allEmbeddingsDF.loc[allEmbeddingsDF['sentence']==sentence,'text']\n",
    "    sentenceDF.reset_index(inplace=True,drop=True)\n",
    "    sentenceText = sentenceDF[0]\n",
    "    print(sentenceText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(set(allPOSDF.subjectsession)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos1DF.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recover all the n-grams containing the target word\n",
    "totalNgramDF = pd.DataFrame()\n",
    "for tokenNum in targetDF['tokenNum']:\n",
    "    ngramDF = speakerDF.iloc[[tokenNum-1,tokenNum,tokenNum+1],:]\n",
    "    totalNgramDF = totalNgramDF.append(ngramDF, ignore_index=True)\n",
    "totalNgramDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakerDF.iloc[[2,3,4],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allPOSDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#                                 Loop through all the sessions and build the allPosFeaturesDF dataframe\n",
    "allSessions = list(set(allPOSDF['subjectsession']))\n",
    "speakers = ['therapist','patient']\n",
    "\n",
    "allPosFeaturesDF = pd.DataFrame(columns=['Session','Speaker','ISYO_count','ISYO_list'])\n",
    "\n",
    "for session in allSessions:\n",
    "    for speaker in speakers:\n",
    "        speakerDF = allPOSDF.loc[(allPOSDF['subjectsession']==session) & (allPOSDF['speaker']==speaker),:].copy()\n",
    "        print('session,speaker: ',session,speaker)\n",
    "        print('speakerDF.shape: ',speakerDF.shape)\n",
    "        speakerDF.reset_index(inplace=True,drop=True)\n",
    "        speakerDF['tokenNum'] = speakerDF.index\n",
    "        #Realign the sentence number\n",
    "        speakerDF['speakerSentenceNum'] = speakerDF['sentence']\n",
    "        speakerDF['speakerSentenceNum'] = speakerDF['speakerSentenceNum'] - speakerDF.loc[0,'speakerSentenceNum']\n",
    "        print(speakerDF.shape)\n",
    "\n",
    "        uposPronDF = speakerDF.loc[(speakerDF['upos']=='PRON'),:]\n",
    "        uposPronounList = list(uposPronDF['lemma'].str.lower())\n",
    "        uposPronounList = [item for item in uposPronounList if item!='covid']\n",
    "        uposPronounSet = list(set(uposPronounList))\n",
    "        sortedPronounList = [item for items, c in Counter(uposPronounList).most_common() for item in [items] * c]\n",
    "\n",
    "        uposIntjDF = speakerDF.loc[(speakerDF['upos']=='INTJ'),:]\n",
    "        uposIntjList = list(uposIntjDF['lemma'].str.lower())\n",
    "        uposIntjList = [item for item in uposIntjList if item!='covid']\n",
    "        uposIntjSet = list(set(uposIntjList))\n",
    "        sortedIntjList = [item for items, c in Counter(uposIntjList).most_common() for item in [items] * c]\n",
    "\n",
    "        \n",
    "        #   Find sentences with various combinations of words\n",
    "        #Find sentence numbers containing the first target word\n",
    "        targetLemma = 'I'\n",
    "        targetDF = speakerDF.loc[(speakerDF['lemma']==targetLemma)&(speakerDF['deprel']=='nsubj'),:]\n",
    "        targetSentenceNumberList = sorted(list(set(targetDF['sentence'])))\n",
    "        #Show the list of sentences containing the target word in its specified role\n",
    "        targetSentencesDF = speakerDF.loc[speakerDF['sentence'].isin(targetSentenceNumberList),:] \n",
    "\n",
    "        #Find sentence numbers containing the second target word\n",
    "        targetLemma = 'you'\n",
    "        targetDF = targetSentencesDF.loc[(targetSentencesDF['lemma']==targetLemma)&(targetSentencesDF['deprel']=='obj'),:]\n",
    "        targetSentenceNumberList = sorted(list(set(targetDF['sentence'])))\n",
    "        #Show the list of sentences containing the target word in its specified role\n",
    "        targetSentencesDF = speakerDF.loc[speakerDF['sentence'].isin(targetSentenceNumberList),:] \n",
    "        newRow = [session, speaker, len(targetSentenceNumberList),targetSentenceNumberList]\n",
    "        allPosFeaturesDF.loc[len(allPosFeaturesDF)] = newRow                 \n",
    "                  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allPosTherapistFeaturesDF = allPosFeaturesDF.loc[allPosFeaturesDF['Speaker']=='therapist'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allPosTherapistFeaturesDF.sort_values(by=['ISYO_count'],inplace=True,ascending=False)\n",
    "allPosTherapistFeaturesDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in allPosTherapistFeaturesDF.iterrows():\n",
    "  print(row['Session'])\n",
    "  isyo_sentenceList = row['ISYO_list']\n",
    "  for sentence in isyo_sentenceList:\n",
    "    sentenceDF = allEmbeddingsDF.loc[allEmbeddingsDF['sentence']==sentence,'text']\n",
    "    sentenceDF.reset_index(inplace=True,drop=True)\n",
    "    sentenceText = sentenceDF[0]\n",
    "    print(sentenceText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
