{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#             Notebook #1\n",
    "#                   Chop Therapy Session transcript files and generate some Features\n",
    "#\n",
    "#              \n",
    "# This notebook reads all the transcripts in '/Users/Heisig/Jihan/Transcripts/' \n",
    "# It creates split subject interviewer transcripts here: /Users/Heisig/Jihan/Transcripts/SplitTranscripts/'\n",
    "# It creates split subject interviewer turn transcripts here: /Users/Heisig/Jihan/Transcripts/SplitTurnsTranscripts'\n",
    "# It creates Google USE embedding vectors and saves them in: '/Users/Heisig/Jihan/Embeddings/'\n",
    "# It creates POS tagged tokens using stanza and saves them in: '/Users/Heisig/Jihan/POS/'\n",
    "# A few plots and stats are available\n",
    "# Then go run LIWC on the split files\n",
    "# Then run the LIWC per Turn Notebook\n",
    "\n",
    "#  Caveat Lector: The notebook is structured for our file system structure and input data types. It will\n",
    "#                 require some refactoring to run on other structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import stanza \n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "from docx.api import Document\n",
    "\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.lm import MLE\n",
    "import contractions\n",
    "from collections import Counter\n",
    "import string\n",
    "from string import punctuation\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "\n",
    "\n",
    "#      Options\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "from notebook.services.config import ConfigManager\n",
    "c = ConfigManager()\n",
    "c.update('notebook', {\"CodeCell\": {\"cm_config\": {\"autoCloseBrackets\": False}}})\n",
    "c.update('notebook', {\"CodeCell\": {\"cm_config\": {\"autoCloseQuotes\": False}}})\n",
    "\n",
    "def plot_similarity(labels, features, rotation):\n",
    "  corr = np.inner(features, features)\n",
    "  sns.set(font_scale=1.2)\n",
    "  g = sns.heatmap(\n",
    "      corr,\n",
    "      xticklabels=labels,\n",
    "      yticklabels=labels,\n",
    "      vmin=0,\n",
    "      vmax=1,\n",
    "      cmap=\"YlOrRd\")\n",
    "  g.set_xticklabels(labels, rotation=rotation)\n",
    "  g.set_title(\"Semantic Textual Similarity\")\n",
    "\n",
    "#Populate the turn DF \n",
    "def create_turnDF_old(all_turns):\n",
    "    speakerDict = {'T':'therapist','P':'tatient'}\n",
    "    turnDF = pd.DataFrame(columns=['Turn','Speaker','Text'])\n",
    "    turn = 0\n",
    "    for i in range(len(all_turns)-1):\n",
    "        currentParts = all_turns[i].text.split(\":\")\n",
    "        if len(currentParts)==2:\n",
    "            print(all_turns[i].text)\n",
    "            speaker = speakerDict[currentParts[0].lstrip()]\n",
    "            text = currentParts[1]\n",
    "            [clean_list,non_verbals] = cleanTurn(text)\n",
    "            print('clean_list: ',clean_list)\n",
    "            print('non_verbals: ',non_verbals)        \n",
    "            #Turn back into a string\n",
    "            clean_text = ' ' .join(clean_list) \n",
    "            newRow = [turn,speaker,clean_text]\n",
    "            turnDF.loc[turn] = newRow\n",
    "            turn = turn + 1\n",
    "    return turnDF\n",
    "\n",
    "#Populate the turnDF and posDF\n",
    "def create_turnDF(all_turns):\n",
    "    #turnDF = pd.DataFrame(columns=['turn','sentence','text'])\n",
    "    speakerDict = {'T':'therapist','P':'patient'}\n",
    "    turnDF = pd.DataFrame(columns=['Turn','Sentence','Speaker','Text'])\n",
    "    turn = 0\n",
    "    posDF = pd.DataFrame(columns=['subjectsession','speaker','sentence','text','lemma','upos','xpos','deprel'])\n",
    "    sentenceTotalNum = 0\n",
    "    for i in range(len(all_turns)):\n",
    "        currentParts = all_turns[i].text.split(\":\")\n",
    "        if len(currentParts)==2:\n",
    "            print(all_turns[i].text)\n",
    "            \n",
    "            #Original version 11/18/21\n",
    "            #speaker = currentParts[0].lstrip()\n",
    "            \n",
    "            #HACK Test 11/18/21???\n",
    "            speaker = speakerDict[currentParts[0].lstrip()]\n",
    "            \n",
    "            \n",
    "            text = currentParts[1]\n",
    "\n",
    "            turn_text = text\n",
    "            \n",
    "            #                         Sentencize and Parse with stanza\n",
    "            #Note once you get the text into a Stanza doc you can do all sorts of interesting things\n",
    "            doc = nlp(turn_text)\n",
    "\n",
    "            sentenceNum = 0\n",
    "            \n",
    "            for sentence in doc.sentences:\n",
    "               turnRow = [turn,sentenceTotalNum,speaker,sentence.text]\n",
    "               turnDF.loc[len(turnDF)+1] = turnRow\n",
    "               \n",
    "               #Populate the POS \n",
    "               for word in sentence.words:\n",
    "                   posDF.loc[len(posDF)+1] = [SubjectSession,speaker,sentenceTotalNum, word.text, word.lemma, word.upos, word.xpos, word.deprel]\n",
    "               sentenceNum = sentenceNum + 1\n",
    "               sentenceTotalNum = sentenceTotalNum + 1\n",
    "               \n",
    "        turn = turn + 1\n",
    "    turnDF.reset_index(inplace=True,drop=True) \n",
    "    print('type(turnDF): ',type(turnDF))\n",
    "    return [turnDF, posDF]\n",
    "\n",
    "#Create the embedding dataframe and write out the patient/therapist text\n",
    "def create_embeddingDF(turnDF):\n",
    "    #Create the dataframe\n",
    "    columnNames = ['subjectsession','speaker','turn','sentence','wordCount','charCount','charsPerWord','text']\n",
    "    columnNames.extend(range(512))\n",
    "    embeddingDF = pd.DataFrame(columns=columnNames)\n",
    "    \n",
    "    embeddings = embed(turnDF['Text'])\n",
    "\n",
    "    for i, message_embedding in enumerate(np.array(embeddings).tolist()):\n",
    "        turnText = turnDF.at[i,'Text']\n",
    "        print('turnText: ',turnText)\n",
    "        speaker = turnDF.at[i,'Speaker']\n",
    "        sentence = turnDF.at[i,'Sentence']\n",
    "        turn = turnDF.at[i,'Turn']\n",
    "        wordCount = len(turnText.split()) \n",
    "        if wordCount==0:\n",
    "            continue\n",
    "        charCount = len(turnText)\n",
    "        charsPerWord = charCount*1.0/wordCount*1.0\n",
    "        newRow = [SubjectSession,speaker,turn,sentence,wordCount,charCount,charsPerWord,turnText]\n",
    "        newRow.extend(message_embedding)\n",
    "        #print(newRow)\n",
    "        embeddingDF.loc[len(embeddingDF)] = newRow\n",
    "        \n",
    "    embeddingFileName = '/Users/Heisig/Jihan/Embeddings/'+SubjectSession+\"_Embedding.csv\"\n",
    "\n",
    "    #embeddingDF.drop('subjectsession', axis=1, inplace=True)\n",
    "    #embeddingDF.drop('turn', axis=1, inplace=True)\n",
    "\n",
    "    embeddingDF.to_csv(embeddingFileName, index=False, header=True, sep=',')\n",
    "\n",
    "    #Now save seperate patient/therapist text files for LIWC processing\n",
    "    patientFileName = '/Users/Heisig/Jihan/Transcripts/SplitTranscripts/'+SubjectSession+\"_patient.txt\"\n",
    "    patientDF = embeddingDF.loc[embeddingDF.speaker=='Subject','text']\n",
    "    patientDF.to_csv(patientFileName, index=False, header=False, sep=',')\n",
    "\n",
    "    therapistFileName = '/Users/Heisig/Jihan/Transcripts/SplitTranscripts/'+SubjectSession+\"_therapist.txt\"\n",
    "    therapistDF = embeddingDF.loc[embeddingDF.speaker=='Interviewer','text']\n",
    "    therapistDF.to_csv(therapistFileName, index=False, header=False, sep=',')\n",
    "    \n",
    "    return embeddingDF\n",
    "\n",
    "def run_and_plot(session_, input_tensor_, messages_, encoding_tensor):\n",
    "  message_embeddings_ = session_.run(\n",
    "      encoding_tensor, feed_dict={input_tensor_: messages_})\n",
    "  plot_similarity(messages_, message_embeddings_, 90)\n",
    "    \n",
    "def cleanTurn(turn):\n",
    "    obfuscated_words = ['xxx','XXX']\n",
    "    \n",
    "    #Save non-verbals\n",
    "    regex = re.compile(\".*?\\((.*?)\\)\")\n",
    "    non_verbals = re.findall(regex, turn)\n",
    "    \n",
    "    #Remove non-verbals\n",
    "    turn = re.sub(r'\\([^)]*\\)', '', turn)\n",
    "   \n",
    "    #Expand contractions\n",
    "    #turn = contractions.fix(turn)\n",
    "    \n",
    "    #Get rid of digits\n",
    "    turn = ''.join([i for i in turn if not i.isdigit()])\n",
    "    \n",
    "    #Get rid of punctuation EXCEPT for periods\n",
    "    #In Jihan's data there actually aren't any periods as sentence delimiters\n",
    "    #There are some elipses\n",
    "    #So using sentence boundaries doesn't make sense\n",
    "    \n",
    "    #Get rid of punctuation EXCEPT for periods\n",
    "    #punctuation_set = punctuation.replace(\".\", \"\")\n",
    "    #turn = turn.translate(str.maketrans(\"\", \"\", punctuation_set))\n",
    "    \n",
    "    #Get rid of ALL punctuation\n",
    "    #turn = turn.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    #Get rid of punctuation EXCEPT for periods, question marks, and exclamation points.\n",
    "    punctuation_set = punctuation.replace(\".\", \"\")\n",
    "    punctuation_set = punctuation_set.replace(\"?\", \"\")\n",
    "    punctuation_set = punctuation_set.replace(\"!\", \"\")\n",
    "    turn = turn.translate(str.maketrans(\"\", \"\", punctuation_set))\n",
    "    \n",
    "    #Switch to lower case\n",
    "    #turn = turn.lower()\n",
    "    \n",
    "    clean_list = turn.split() \n",
    "    \n",
    "    #Get rid of obfuscated words\n",
    "    clean_list = [word for word in clean_list if not word in obfuscated_words]\n",
    "    \n",
    "    return [clean_list,non_verbals]\n",
    "\n",
    "def cleanSentence(sentence):\n",
    "    obfuscated_words = ['XXX','xxx','ykwis','ykis','ykwim',]\n",
    "    \n",
    "    #Save non-verbals\n",
    "    regex = re.compile(\".*?\\((.*?)\\)\")\n",
    "    non_verbals = re.findall(regex, sentence)\n",
    "    \n",
    "    #Remove non-verbals\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence)\n",
    "   \n",
    "    #Expand contractions\n",
    "    #sentence = contractions.fix(sentence)\n",
    "    \n",
    "    #Get rid of digits\n",
    "    sentence = ''.join([i for i in sentence if not i.isdigit()])\n",
    "\n",
    "    #Get rid of punctuation EXCEPT for periods, question marks, and exclamation points.\n",
    "    punctuation_set = punctuation.replace(\".\", \"\")\n",
    "    punctuation_set = punctuation_set.replace(\"?\", \"\")\n",
    "    punctuation_set = punctuation_set.replace(\"!\", \"\")\n",
    "    turn = turn.translate(str.maketrans(\"\", \"\", punctuation_set))\n",
    "    \n",
    "    #Switch to lower case\n",
    "    #sentence = sentence.lower()\n",
    "    \n",
    "    clean_list = sentence.split() \n",
    "    \n",
    "    #Get rid of obfuscated words\n",
    "    clean_list = [word for word in clean_list if not word in obfuscated_words]\n",
    "    \n",
    "    clean_sentence = ' '.join(clean_list)\n",
    "    \n",
    "    return clean_sentence\n",
    "\n",
    "\n",
    "def ngramStats(allTextList,n):\n",
    "    #Compute word frequencies\n",
    "    word_fdist = nltk.FreqDist(allTextList)    \n",
    "    wordDF = pd.DataFrame.from_dict(word_fdist, orient='index')\n",
    "    wordDF['word'] = wordDF.index\n",
    "    wordDF.columns = ['count','word']  \n",
    "    wordDF = wordDF[['word','count']] \n",
    "    wordDF.sort_values(by=['count'], ascending=False, inplace=True)\n",
    "    wordDF.reset_index(inplace=True,drop=True)\n",
    "    wordDF.head()\n",
    "\n",
    "    #Compute n gram frequencies\n",
    "    ngram_fd = nltk.FreqDist(ngrams(allTextList,n))\n",
    "    ngramDF = pd.DataFrame(ngram_fd.most_common(), columns =['w1-wn', 'Freq']) \n",
    "    return[wordDF,ngramDF]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#       Pull in the model\n",
    "#Off world\n",
    "#embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "#Local\n",
    "embed = hub.load('/Users/Heisig/Jihan/USE/universal-sentence-encoder_4') \n",
    "stanza.download('en') # download English model\n",
    "#Initialize a Stanza Pipeline\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,lemma,pos,depparse,ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read all the split transcripts and generate per turn files for LIWC\n",
    "# Generate allTurnsTextDF\n",
    "\n",
    "documentDF = pd.DataFrame(columns=['SubjectSession','document'])\n",
    "                          \n",
    "InputDocumentDir = '/Users/Heisig/Jihan/Transcripts/'\n",
    "turnsDir = '/Users/Heisig/Jihan/Transcripts/SplitTurnsTranscripts/'\n",
    "                          \n",
    "file_paths = []  # List which will store all of the full filepaths.\n",
    "# Walk the directory tree.\n",
    "for root, directories, files in os.walk(InputDocumentDir):\n",
    "    for filename in files:\n",
    "        # Join the two strings in order to form the full filepath.\n",
    "        filepath = os.path.join(root, filename)\n",
    "        #print('root: ',root)\n",
    "        #print('filename: ',filename)\n",
    "        file_paths.append(filepath)  # Add it to the list.\n",
    "        \n",
    "docxFiles  = [s for s in file_paths if '.docx' in s]\n",
    "print('docxFiles: \\n',docxFiles)\n",
    "\n",
    "allTurnsTextDF = pd.DataFrame(columns=['SubjectSession', 'Speaker', 'Turn', 'Text'])\n",
    "\n",
    "for docxFile in docxFiles:   \n",
    "    document = Document(docxFile)\n",
    "    all_turns = document.paragraphs\n",
    "    path_file = os.path.split(docxFile) \n",
    "    fileName = path_file[1]\n",
    "    fileParts = fileName.split('.')\n",
    "    SubjectSession = fileParts[0]\n",
    "\n",
    "    #Create the turns dataframe\n",
    "    [turnDF, posDF] = create_turnDF(all_turns)\n",
    "    turnDF.insert(0, 'SubjectSession', [SubjectSession]*len(turnDF))\n",
    "    allTurnsTextDF = allTurnsTextDF.append(turnDF, ignore_index=True)\n",
    "    \n",
    "    patientTurnsDF = turnDF.loc[turnDF['Speaker']=='Subject']\n",
    "    patientTurnsDF.reset_index(inplace=True,drop=True) \n",
    "    patientTurns, cols = patientTurnsDF.shape\n",
    "    interviewerTurnsDF = turnDF.loc[turnDF['Speaker']=='Interviewer']\n",
    "    interviewerTurnsDF.reset_index(inplace=True,drop=True)\n",
    "    interviewerTurns, cols = interviewerTurnsDF.shape\n",
    "    totalTurns = patientTurns + interviewerTurns\n",
    "    \n",
    "    #Write each turn to its own file so LIWC can pick it up individually\n",
    "    sessionDir = turnsDir+SubjectSession+'/'\n",
    "    if os.path.isdir(sessionDir):\n",
    "       print(sessionDir + ' already exists')\n",
    "    else: \n",
    "       os.mkdir(sessionDir)\n",
    "       print(sessionDir + ' created') \n",
    "     \n",
    "    for index, row in patientTurnsDF.iterrows():\n",
    "        outFileName = sessionDir+SubjectSession+'_patient_'+str(totalTurns)+'_'+str(row['Sentence'])+'.txt'\n",
    "        outF = open(outFileName, 'w')\n",
    "        outF.write(row['Text'])\n",
    "        outF.close()    \n",
    "    for index, row in interviewerTurnsDF.iterrows():\n",
    "        outFileName = sessionDir+SubjectSession+'_interviewer_'+str(totalTurns)+'_'+str(row['Sentence'])+'.txt'\n",
    "        outF = open(outFileName, 'w')\n",
    "        outF.write(row['Text'])\n",
    "        outF.close()  \n",
    "\n",
    "#Fixup for merge with LIWC and Clinical later\n",
    "speakerDict = {'Interviewer' : 'interviewer', 'Subject' : 'patient'} \n",
    "allTurnsTextDF.replace({\"Speaker\": speakerDict},inplace=True)\n",
    "allTurnsTextDF['SubjectSession'] = allTurnsTextDF['SubjectSession'].astype('int')\n",
    "allTurnsTextDF['SubjectSession'] = allTurnsTextDF['SubjectSession'].astype('str')\n",
    "\n",
    "\n",
    "#Save the turn text file\n",
    "turnTextFileName = '/Users/Heisig/Jihan/LIWC/TurnText.csv'\n",
    "allTurnsTextDF.to_csv(turnTextFileName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                 Process all the transcripts New Style (whole turn)\n",
    "\n",
    "\n",
    "# Load the first table from your document. In your example file,\n",
    "# there is only one table, so I just grab the first one.\n",
    "\n",
    "\n",
    "documentDF = pd.DataFrame(columns=['SubjectSession','document'])\n",
    "                          \n",
    "InputDocumentDir = '/Users/Heisig/Jihan/Transcripts/'\n",
    "StatsDir = '/Users/Heisig/Jihan/Stats/'\n",
    "                          \n",
    "file_paths = []  # List which will store all of the full filepaths.\n",
    "# Walk the directory tree.\n",
    "for root, directories, files in os.walk(InputDocumentDir):\n",
    "    for filename in files:\n",
    "        # Join the two strings in order to form the full filepath.\n",
    "        filepath = os.path.join(root, filename)\n",
    "        #print('root: ',root)\n",
    "        #print('filename: ',filename)\n",
    "        file_paths.append(filepath)  # Add it to the list.\n",
    "        \n",
    "docxFiles = sorted([s for s in file_paths if \".docx\" in s])\n",
    "\n",
    "allAllPatientList = []\n",
    "allAllTherapistList = []   \n",
    " \n",
    "    \n",
    "for docxFile in docxFiles:\n",
    "    document = Document(docxFile)\n",
    "    all_turns = document.paragraphs\n",
    "    path_file = os.path.split(docxFile) \n",
    "    fileName = path_file[1]\n",
    "    fileParts = fileName.split('.')\n",
    "    SubjectSession = fileParts[0]\n",
    "    \n",
    "    #Create the turnDF \n",
    "    print('SubjectSession: ',SubjectSession)\n",
    "    [turnDF,posDF] = create_turnDF(all_turns)\n",
    "    \n",
    "    speakerDict = {'Interviewer' : 'therapist', 'Subject' : 'patient'} \n",
    "    allTurnsTextDF.replace({\"Speaker\": speakerDict},inplace=True)\n",
    "    \n",
    "    #Save the POS files\n",
    "    posFileName = '/Users/Heisig/Jihan/POS/'+SubjectSession+\"_POS.csv\"\n",
    "    posDF.to_csv(posFileName, index=False, header=True, sep=',')\n",
    "\n",
    "    \n",
    "    #Create (and write out) the embeddings for each turn\n",
    "    embeddingDF = create_embeddingDF(turnDF)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "patientEmbeddingDF = embeddingDF.loc[embeddingDF['speaker']=='patient']\n",
    "therapistEmbeddingDF = embeddingDF[embeddingDF['speaker']=='therapist']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(16, 8)\n",
    "\n",
    "\n",
    "plt.hist([patientEmbeddingDF[0], therapistEmbeddingDF[0]], \n",
    "         bins=100,\n",
    "         color=['r','b'], \n",
    "         label=['patient','therapist'],\n",
    "         alpha=0.5)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "#sns.distplot(vector, label=row,bins=250, ax=ax, hue='speaker')\n",
    "#plotTitle = 'embedding vector '+str(row)\n",
    "#sns.distplot(vector, label=row,bins=250,ax=ax).set_title(plotTitle)\n",
    "#plt.legend();\n",
    "#ax.set(xlabel='PreOp '+metric, ylabel='Observations')\n",
    "#ax.set_xlim(0,1)\n",
    "  \n",
    "#plotFileNPree = outputDir+Subject+\"_\"+metric+\"_Histo.png\"\n",
    "#fig.savefig(plotFileNPree)\n",
    "#plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allPatientStr = patientEmbeddingDF['text'].str.cat(sep=' ')\n",
    "print('allPatientStr: \\n',allPatientStr)\n",
    "allPatientList = allPatientStr.split()\n",
    "n = 4\n",
    "wordDF,ngramDF = ngramStats(allPatientList,n)\n",
    "print('ngramDF: \\n',ngramDF.head(n=25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTherapistStr = therapistEmbeddingDF['text'].str.cat(sep=' ')\n",
    "print('allTherapistStr : \\n',allTherapistStr )\n",
    "allTherapistList = allTherapistStr.split()\n",
    "n = 4\n",
    "wordDF,ngramDF = ngramStats(allTherapistList,n)\n",
    "print('ngramDF: \\n',ngramDF.head(n=25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count therapist words\n",
    "print('Total therapist words: ',len(allTherapistList)) \n",
    "therapist_word_fdist = nltk.FreqDist(allTherapistList) \n",
    "therapist_fdistDF = pd.DataFrame.from_dict(therapist_word_fdist, orient='index')\n",
    "therapist_fdistDF['word'] = therapist_fdistDF.index\n",
    "therapist_fdistDF.columns = ['therapist_count','word'] \n",
    "therapist_fdistDF['therapist_freq'] = therapist_fdistDF['therapist_count']/len(allTherapistList)\n",
    "therapist_fdistDF.sort_values(by=['therapist_count'], ascending=False, inplace=True)\n",
    "therapist_fdistDF.reset_index(inplace=True,drop=True)\n",
    "therapist_fdistDF.head(n=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count patient words\n",
    "print('Total patient words: ',len(allPatientList)) \n",
    "patient_word_fdist = nltk.FreqDist(allPatientList) \n",
    "patient_fdistDF = pd.DataFrame.from_dict(patient_word_fdist, orient='index')\n",
    "patient_fdistDF['word'] = patient_fdistDF.index\n",
    "patient_fdistDF.columns = ['patient_count','word'] \n",
    "patient_fdistDF['patient_freq'] = patient_fdistDF['patient_count']/len(allPatientList)\n",
    "patient_fdistDF.sort_values(by=['patient_count'], ascending=False, inplace=True)\n",
    "patient_fdistDF.reset_index(inplace=True,drop=True)\n",
    "patient_fdistDF.head(n=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_fdistDF = pd.merge(patient_fdistDF,therapist_fdistDF, on='word')\n",
    "total_fdistDF.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Patient and Therapist Freq Word freq\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(12, 8)\n",
    "plt.gcf().subplots_adjust(bottom=0.15)\n",
    "\n",
    "ax = sns.scatterplot(data=total_fdistDF,\n",
    "                     x=\"patient_freq\",\n",
    "                     y=\"therapist_freq\");\n",
    "ax.set_xlim(0,0.09)\n",
    "ax.set_ylim(0,0.09)\n",
    "\n",
    "for lineNum in range(0,25):\n",
    "             pointLabel = total_fdistDF.word[lineNum]\n",
    "             ax.text(total_fdistDF.loc[lineNum,'patient_freq'], total_fdistDF.loc[lineNum,'therapist_freq'], pointLabel, horizontalalignment='left', size='small', alpha=0.99, color='black', weight='roman')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of all .docx files \n",
    "allDocxFiles = [s for s in file_paths if '.docx' in s]\n",
    "print(allDocxFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#                  Old Style Comma and Period Parser\n",
    "\n",
    "#Single fle HACK\n",
    "allDocxFiles = ['/Users/Heisig/Jihan/Transcripts/030920.docx']\n",
    "\n",
    "for docFile in allDocxFiles:\n",
    "\n",
    "    path_file = os.path.split(docxFile) \n",
    "    fileName = path_file[1]\n",
    "    fileParts = fileName.split('.')\n",
    "    SubjectSession = fileParts[0]\n",
    "    \n",
    "    \n",
    "    document = Document(docFile)\n",
    "    all_turns = document.paragraphs\n",
    "    parts = docFile.split('/')\n",
    "    print(parts[5])\n",
    "    path_file = os.path.split(docFile) \n",
    "    fileName = path_file[1]\n",
    "    fileParts = fileName.split('.')\n",
    "    SubjectSession = fileParts[0]\n",
    "\n",
    "    #Populate the turn DF \n",
    "    speakerDict = {'T':'Interviewer','P':'Subject'}\n",
    "    turnDF = pd.DataFrame(columns=['Turn','Speaker','Text'])\n",
    "    turn = 0\n",
    "    for i in range(len(all_turns)-1):\n",
    "        currentParts = all_turns[i].text.split(\":\")\n",
    "        if len(currentParts)==2:\n",
    "            #print(all_turns[i].text)\n",
    "            speaker = speakerDict[currentParts[0]]\n",
    "            text = currentParts[1]\n",
    "            newRow = [turn,speaker,text]\n",
    "            turnDF.loc[turn] = newRow\n",
    "            turn = turn + 1\n",
    "    sentenceList = []\n",
    "    sentenceSourceList = []\n",
    "    parseType = 'Commas'\n",
    "    #parseType = 'Sentences'\n",
    "    for index, row in turnDF.iterrows():\n",
    "        turn = row['Text']\n",
    "        start = turn.find( '(' )\n",
    "        end = turn.find( ')' )\n",
    "        if start != -1 and end != -1:\n",
    "           turn = turn[start+1:end]\n",
    "        turn = turn.replace('(','')\n",
    "        turn = turn.replace(')','')\n",
    "        if parseType == 'Commas':\n",
    "            turn = turn.replace(',','.')\n",
    "        else:\n",
    "            turn = turn.replace(',','')\n",
    "        turn = turn.replace('?','.')\n",
    "        turn = turn.replace('!','.')\n",
    "        #print('turn: ',turn)\n",
    "        sentences = turn.split('.')\n",
    "        #print('sentences: ',sentences)\n",
    "        while(\"\" in sentences): \n",
    "            sentences.remove(\"\")\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            sentence = cleanSentence(sentence)\n",
    "            if len(sentence)>1:\n",
    "                sentenceList.append(sentence)\n",
    "                sentenceSourceList.append(row['Speaker'])\n",
    "                print(sentence)\n",
    "        \n",
    "    #Create the dataframe\n",
    "    columnNames = ['subjectsession','speaker','turn','text','wordCount','charCount','charsPerWord']\n",
    "    columnNames.extend(range(512))\n",
    "    print('len(columnNames): ',len(columnNames))\n",
    "    embeddingDF = pd.DataFrame(columns=columnNames)\n",
    "\n",
    "    embeddings = embed(sentenceList)\n",
    "\n",
    "    for i, message_embedding in enumerate(np.array(embeddings).tolist()):\n",
    "        #print(message_embedding)\n",
    "        wordCount = len(sentenceList[i].split()) \n",
    "        charCount = len(sentenceList[i])\n",
    "        charsPerWord = charCount*1.0/wordCount*1.0\n",
    "        newRow = [SubjectSession,sentenceSourceList[i],i,sentenceList[i],wordCount,charCount,charsPerWord]\n",
    "        newRow.extend(message_embedding)\n",
    "        #print(newRow)\n",
    "        #print(len(newRow),len(newRow))\n",
    "        embeddingDF.loc[len(embeddingDF)] = newRow\n",
    "    embeddingFileName = '/Users/Heisig/Jihan/OldStyleEmbeddings/'+SubjectSession+\"_\"+parseType+\"_Embedding.csv\"\n",
    "\n",
    "    embeddingDF.drop('subjectsession', axis=1, inplace=True)\n",
    "    #embeddingDF.drop('turn', axis=1, inplace=True)\n",
    "\n",
    "    embeddingDF.to_csv(embeddingFileName, index=False, header=True, sep=',')\n",
    "    \"\"\"\n",
    "    #Now save seperate patient/therapist text files for LIWC processing\n",
    "    patientFileName = '/Users/Heisig/Jihan/Transcripts/'+SubjectSession+\"_patient.txt\"\n",
    "    patientDF = embeddingDF.loc[embeddingDF.speaker=='Subject','text']\n",
    "    patientDF.to_csv(patientFileName, index=False, header=True, sep=',')\n",
    "\n",
    "    therapistFileName = '/Users/Heisig/Jihan/Transcripts/'+SubjectSession+\"_therapist.txt\"\n",
    "    therapistDF = embeddingDF.loc[embeddingDF.speaker=='Interviewer','text']\n",
    "    therapistDF.to_csv(therapistFileName, index=False, header=True, sep=',')\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_fdist = nltk.FreqDist(sentence_list) \n",
    "fdistDF = pd.DataFrame.from_dict(word_fdist, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allAllTherapistList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patientEmbeddingDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read a previously unseen session\n",
    "docxFile = '/Users/Heisig/Jihan/Validation_Transcript/111920.docx'\n",
    "print('docxFile: ',docxFile)\n",
    "document = Document(docxFile)\n",
    "path_file = os.path.split(docxFile) \n",
    "fileName = path_file[1]\n",
    "fileParts = fileName.split('.')\n",
    "SubjectSession = fileParts[0]\n",
    "\n",
    "all_turns = document.paragraphs\n",
    "\n",
    "#This returns 'cleaned' turn text for each speaker\n",
    "#Commas really aren't a legit way to split the text further\n",
    "#So we will either use the whole turn or ngram our way through\n",
    "#it.\n",
    "validity_turnDF = create_turnDF(all_turns)\n",
    "validity_therapistDF = embeddingDF.loc[embeddingDF.speaker=='Interviewer',:]\n",
    "validity_patientDF = embeddingDF.loc[embeddingDF.speaker=='Subject',:]\n",
    "print(validity_patientDF.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Unigram Train/Test Perplexity Evaluation\n",
    "\n",
    "import nltk\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "\n",
    "# Jihan Patient, Therapist Training data\n",
    "tokenized_patient_text = [list(map(str.lower, nltk.tokenize.word_tokenize(sent))) \n",
    "                for sent in patientEmbeddingDF['text']]\n",
    "\n",
    "tokenized_therapist_text = [list(map(str.lower, nltk.tokenize.word_tokenize(sent))) \n",
    "                for sent in therapistEmbeddingDF['text']]\n",
    "\n",
    "tokenized_patient_validity_text = [list(map(str.lower, nltk.tokenize.word_tokenize(sent))) \n",
    "                for sent in validity_patientDF['text']]\n",
    "\n",
    "tokenized_therapist_validity_text = [list(map(str.lower, nltk.tokenize.word_tokenize(sent))) \n",
    "                for sent in validity_therapistDF['text']]\n",
    "\n",
    "#Train unigram model on patient speech\n",
    "n = 1\n",
    "train_data, padded_vocab = padded_everygram_pipeline(n, tokenized_patient_text)\n",
    "patient_model = MLE(n)\n",
    "patient_model.fit(train_data, padded_vocab)\n",
    "\n",
    "\n",
    "#Train unigram model on therapist speech\n",
    "n = 1\n",
    "train_data, padded_vocab = padded_everygram_pipeline(n, tokenized_therapist_text)\n",
    "therapist_model = MLE(n)\n",
    "therapist_model.fit(train_data, padded_vocab)\n",
    "\n",
    "\n",
    "#Evaluate the patient validity transcript using the Patient model MLE and PP\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('Patient Validity vs. Patient Model') \n",
    "test_data, _ = padded_everygram_pipeline(n, tokenized_patient_validity_text)\n",
    "for test in test_data:\n",
    "    print (\"MLE Estimates:\", [((ngram[-1], ngram[:-1]),patient_model.score(ngram[-1], ngram[:-1])) for ngram in test])\n",
    "for i, test in enumerate(test_data):\n",
    "  print(\"PP({0}):{1}\".format(print('test_data: ',test_data)[i], patient_model.perplexity(test)))\n",
    "\n",
    "#Evaluate the patient validity transcript using Therapist model MLE and PP\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "print('Patient Validity vs. Patient and Therapist Models')\n",
    "test_data, _ = padded_everygram_pipeline(n, tokenized_patient_validity_text)\n",
    "for test in test_data:\n",
    "    print (\"MLE Estimates:\", [((ngram[-1], ngram[:-1]),therapist_model.score(ngram[-1], ngram[:-1])) for ngram in test])\n",
    "\n",
    "test_data, _ = padded_everygram_pipeline(n, tokenized_patient_validity_text)\n",
    "for i, test in enumerate(test_data):\n",
    "  print(\"PP({0}):{1}\".format(tokenized_patient_validity_text[i], therapist_model.perplexity(test)))\n",
    "\n",
    "\n",
    "#Evaluate vs. therapist\n",
    "test_data, _ = padded_everygram_pipeline(n, tokenized_therapist_validity_text)\n",
    "for test in test_data:\n",
    "    print (\"MLE Estimates:\", [((ngram[-1], ngram[:-1]),patient_model.score(ngram[-1], ngram[:-1])) for ngram in test])\n",
    "\n",
    "test_data, _ = padded_everygram_pipeline(n, tokenized_therapist_validity_text)\n",
    "for i, test in enumerate(test_data):\n",
    "  print(\"PP({0}):{1}\".format(tokenized_therapist_validity_text[i], therapist_model.perplexity(test)))\n",
    "\n",
    "\n",
    "\n",
    "#Evaluate Hand-whittled sentences vs. Therapist Model\n",
    "print('/n')\n",
    "print('/n')\n",
    "print('Test Sentences vs. Therapist Model')\n",
    "test_sentences = ['i am really tired', 'i do not know', 'you could think about it']\n",
    "tokenized_test_text = [list(map(str.lower, nltk.tokenize.word_tokenize(sent))) \n",
    "                for sent in test_sentences]\n",
    "test_data, _ = padded_everygram_pipeline(n, tokenized_test_text)\n",
    "for test in test_data:\n",
    "    print (\"MLE Estimates:\", [((ngram[-1], ngram[:-1]),therapist_model.score(ngram[-1], ngram[:-1])) for ngram in test])\n",
    "\n",
    "test_data, _ = padded_everygram_pipeline(n, tokenized_test_text)\n",
    "for i, test in enumerate(test_data):\n",
    "  print(\"PP({0}):{1}\".format(tokenized_test_text[i], therapist_model.perplexity(test)))\n",
    "\n",
    "\n",
    "#Evaluate Hand-whittled sentences vs. Patient Model\n",
    "print('/n')\n",
    "print('/n')\n",
    "print('Test Sentences vs. Patient Model')\n",
    "test_sentences = ['i am really tired', 'i do not know', 'you could think about it']\n",
    "tokenized_test_text = [list(map(str.lower, nltk.tokenize.word_tokenize(sent))) \n",
    "                for sent in test_sentences]\n",
    "test_data, _ = padded_everygram_pipeline(n, tokenized_test_text)\n",
    "for test in test_data:\n",
    "    print (\"MLE Estimates:\", [((ngram[-1], ngram[:-1]),patient_model.score(ngram[-1], ngram[:-1])) for ngram in test])\n",
    "\n",
    "test_data, _ = padded_everygram_pipeline(n, tokenized_test_text)\n",
    "for i, test in enumerate(test_data):\n",
    "  print(\"PP({0}):{1}\".format(tokenized_test_text[i], patient_model.perplexity(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_patient_validity_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vanilla Model\n",
    "\n",
    "import nltk\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n",
    "\n",
    "train_sentences = ['an apple', 'an orange']\n",
    "tokenized_text = [list(map(str.lower, nltk.tokenize.word_tokenize(sent))) \n",
    "                for sent in train_sentences]\n",
    "n = 1\n",
    "train_data, padded_vocab = padded_everygram_pipeline(n, tokenized_text)\n",
    "model = MLE(n)\n",
    "model.fit(train_data, padded_vocab)\n",
    "\n",
    "test_sentences = ['an apple', 'an ant']\n",
    "tokenized_text = [list(map(str.lower, nltk.tokenize.word_tokenize(sent))) \n",
    "                for sent in test_sentences]\n",
    "\n",
    "test_data, _ = padded_everygram_pipeline(n, tokenized_text)\n",
    "for test in test_data:\n",
    "    print (\"MLE Estimates:\", [((ngram[-1], ngram[:-1]),model.score(ngram[-1], ngram[:-1])) for ngram in test])\n",
    "\n",
    "test_data, _ = padded_everygram_pipeline(n, tokenized_text)\n",
    "\n",
    "for i, test in enumerate(test_data):\n",
    "  print(\"PP({0}):{1}\".format(test_sentences[i], model.perplexity(test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
